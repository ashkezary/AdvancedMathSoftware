{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate all urls we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(1560):\n",
    "    urls.append(\"https://www.golha.co.uk/fa/programme/%d\"%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract data from target webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('golha.csv', mode='w',newline='',encoding='utf-8-sig')\n",
    "golha = csv.writer(f, delimiter=',')\n",
    "for url in urls:\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    mysoup = soup.div(id='programme-people')[0]\n",
    "    foo1 = mysoup.h1.text\n",
    "    names = mysoup.find_all(\"strong\")\n",
    "    musis = []\n",
    "    for x in names:\n",
    "        musis.append(x.text)\n",
    "    if 'گوینده رادیو' in musis:\n",
    "        musis.remove('گوینده رادیو')\n",
    "    mydivs = soup.find_all(\"div\", {\"class\": \"item_div\"})\n",
    "    poem = \"\"\n",
    "    singer = \"\"\n",
    "    for div in mydivs: \n",
    "        d = div.text\n",
    "        x = d.find(\"مطلع شعر آواز: \")\n",
    "        y = d.find(\"خواننده آواز: \")\n",
    "        if x>0:\n",
    "            poem = d[x+15:]\n",
    "            singer = d[y+14:].split(' ')[0]\n",
    "            break;\n",
    "    try:\n",
    "        strs = mydivs[5].text\n",
    "        regex = r\"\\([^\\)]*\\)\"\n",
    "        rah = re.findall(regex,strs)\n",
    "        golha.writerow([url,foo1,musis,rah[0],poem,singer])\n",
    "    except:\n",
    "        pass\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all the praises and thanks be to Allah, the Lord of the worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
